Notoriously, three-stage paradigm for training a GLM(Generative Language Model)
1. **Pretraining:** unsupervised pretraining from raw text, to learn general-purpose representations
2. **Instruction tuning:** aligning Language Models with instruction format data
4. **RLHF:** use human feedback and reinforcement learning technique to better align to end tasks and user preferences.
Not matter pretraining stage, instruction tuning or RLHF stage of LLM, how to mix and utilize data is the key point of model performance. 



## 📃Related Papers






## ☑️Todo List
